# claimcall-ai-voicebot

## ðŸ“¹ Demo

[![Watch the demo](./asset/thumb-pipe.png)](https://drive.google.com/file/d/10mptEP5FyJSIosPZOXPIyJIV6jq0KMrU/view?usp=sharing)

The project implements an AI conversation agent that handles voice-based interactions in a healthcare collection scenario. It follows the classic pipeline of Speech-to-Text (STT) â†’ Language Model (LLM) â†’ Text-to-Speech (TTS) to process spoken queries and generate spoken responses.
In practice, a user speaks to the system (captured via a browser or phone), the audio is streamed over WebRTC, a VAD (voice activity detector) segments the speech, and an STT model transcribes the speech to text. This text is then passed into a large language model (e.g. OpenAIâ€™s GPT) which produces a text reply, and finally a TTS engine synthesizes that reply back into audio. Pipecat â€“ an open-source Python framework â€“ orchestrates this real-time audio/AI flow. In healthcare contexts, such voice agents are used for tasks like patient payment reminders and overdue insurance clarifications, demonstrating the relevance of this demo system.

### System Architecture and Pipeline
- **Audio Streaming (WebRTC):** Real-time bidirectional audio transport via WebRTC enables low-latency communication between user and server. The browser or client sends live audio to the Pipecat backend in real time.
- **Voice Activity Detection (VAD):** A VAD (e.g. Silero VAD) monitors the stream and flags when the user starts or stops speaking. This lets the system trim silence and process only the spoken segments.
- **Speech-to-Text (ASR):** Each detected speech segment is converted to text by a speech recognition model. We evaluated NVIDIA Riva for ASR â€“ Riva is a production-ready suite of ASR/TTS services praised for high accuracy. Pipecat also supports providers like AssemblyAI, Google, AWS, Cartesia, OpenAI, etc.
- **Large Language Model:** The transcribed text is sent to an LLM (e.g. OpenAIâ€™s GPT-4). The LLM interprets the userâ€™s query and any contextual history to generate an appropriate response. We used the OpenAI API as the LLM backend.
- **Text-to-Speech (TTS):** The LLMâ€™s text reply is fed into a TTS engine to synthesize audio. We tested ElevenLabs, NVIDIA Riva TTS, and Cartesia. ElevenLabs stood out for naturalness â€“ it offers thousands of voices in 70+ languages with superior realism. Pipecat supports ElevenLabs, Riva, Cartesia and others for TTS. For example, ElevenLabs can clone voices in seconds and has minimal latency (â‰ˆ75 ms).
- **Audio Playback:** The synthesized speech audio is then streamed back to the user over WebRTC and played through their speaker, completing the conversational turn.

This cascaded voice AI pipeline (STTâ†’LLMâ†’TTS) is a well-established design. In fact, a Pipecat example in AWSâ€™s reference architectures precisely uses this sequence (WebRTC â†’ VAD â†’ ASR â†’ LLM â†’ TTS) to handle voice queries. We followed this model closely: each component is modular, allowing us to plug in different ASR or TTS services as needed.

### Technology Stack
- **Pipecat Framework:** We used Pipecat as the core framework. Pipecat is an open-source Python library for real-time multimodal agents. It is voice-first (built to integrate STT, TTS, and conversational logic) and pluggable (supports many AI services). Its design (and official examples) greatly accelerated development. We also considered Pipecat Flows, an add-on for scripting structured dialogues, to handle domain-specific conversation logic (e.g. guiding a payment reminder flow).
- **Python & uv:** The project is written in Python. We managed the environment and dependencies with uv â€“ a modern Python project manager written in Rust. Using uv init, the project setup (including a pyproject.toml) was created quickly. The uv tool streamlines creating the virtual environment and packaging, so setup instructions are based on it.
- **Transport & Endpoints:** Pipecatâ€™s default transport is used: a WebRTC-based endpoint for audio (it can connect to a Daily.co or LiveKit room under the hood). We also used a FastAPI/- - WebSocket server for signaling and control. No external telephony service was needed; all communication happens via browser.
- **ASR Service:** We plugged in NVIDIA Riva for speech-to-text. Riva is tailored for low-latency, on-prem or cloud ASR with strong multilingual support. We also experimented with Cartesia (a newer provider) for ASR. Pipecatâ€™s service list confirms support for Riva, Cartesia, and others.
- **LLM Backend:** The system uses OpenAIâ€™s GPT API as the language model (via Pipecatâ€™s OpenAI service integration). The model is driven by the userâ€™s prompt and flows through the conversation logic. TTS Engine: For voice output, we tried ElevenLabs, NVIDIA Riva TTS, and Cartesia. ElevenLabs gave the most natural results in tests; Pipecat natively supports all of them. The user just specifies the chosen TTS voice ID in the config.
- **Voice Activity Detection:** Pipecat includes Silero VAD and other audio tools. We used VAD to automatically segment turns so the agent knows when to respond 